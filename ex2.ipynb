{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Astro 528, Lab 7, Exercise 2\n## GPU Computing II:  Broadcasting, Fusion, Reductions\n\nIn this lab exercise, we'll build on programming pattern for harnessing GPUs with a very high-level interface. This assumes that you've successfully gotten GPUs working with Julia from [exercise 1](ex1.ipynb).  Again, students are advised to run the exercises in this lab on ICS-ACI rather than their own system.  \n\nThe ICS-ACI Jupyter notebook server and interactive desktop provided by the ICS-ACI portal are now using interactive notes that include a GPU, and students should be able to run all the code there.  However, the GPUs on the interactive nodes are relatively modest GPUs.  You may want to go ahead and submit ex2.pbs, so that it can queue and run on a CyberLAMP GPU node while you work through this notebook.  Then you can review the output to compare the results and performance of the GPUs on the interactive node to that of the CyberLAMP GPU nodes.  For submitting jobs, you'll use the [command line interface](https://ics.psu.edu/computing-services/ics-aci-user-guide/#05-00-basics-aci-resources) to submit the PBS jobs, following a similar syntax as the [lab's 6 README](https://github.com/PsuAstro528/lab6-start/blob/master/README.md).  \n\n## Setting up\nFirst, we'll make sure that `CUDAHOME` is set, activate the project in the current directory and load packages to be used. Here we assume that you've already instantiated this project during exercise 1."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "if haskey(ENV,\"CUDAHOME\")\n    println(\"The CUDAHOME environment variable has been set to: \",ENV[\"CUDAHOME\"])\nelse\n    println(\"The CUDAHOME environment variable has not been set.  Setting it now...\")\n    ENV[\"CUDAHOME\"] = \"/gpfs/group/ebf11/default/astro528/cuda\"\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Pkg\nPkg.activate(\".\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CUDAnative, CuArrays, GPUArrays"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using BenchmarkTools, Statistics"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#pyplot()  # in case gr() gives you trouble"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing GPU-compatiable functions for broadcasting\n\n\nYou can write high-level julia code for GPUs even if they have more than linear algebra.  Here I'll show a demonstration of solving [Kepler's equation](https://en.wikipedia.org/wiki/Kepler%27s_equation) and computing the radial velocity of a star due to its reflex motion due to a planet following a Keplerian orbit.  Kepler's equation describes the motion in time of a body on a Keplerian orbit.  The \"mean anomaly\" (M) increases linearly with time, while the \"eccentric anomaly\" (E) gives the angular position of the body.  Kepler's equation is $$M = E - e \\sin(E),$$ where $M$ is the mean anomaly, $E$ is the eccentric anomaly, and $e$ is the orbital eccentricity.  While it is trivial to compute the mean anomaly from the eccentric anomaly, computing the eccentric anomaly at a given time requires an itterative solver, which makes this relatively slow (compared to other parts of orbit calculation) and hence a good candidate for implementing on a GPU.  \n\nTo keep the notebook relatively clean, the code for solving Kepler's equation is in [src/kepler_eqn.jl](src/kepler_eqn.jl).  First, let's load that code and its module."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "include(\"src/kepler_eqn.jl\")\nusing .KeplerEqn"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a simple 2-d grid of values of the eccentricity and mean anomaly.  (Feel free to remove the `;`'s to inspect the results.)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "num_ecc = 100\nnum_mean_anom = 200\nmax_ecc = 0.9\necc_h = [max_ecc*i/num_ecc for i in 0:(num_ecc-1), j in 0:(num_mean_anom-1) ]\nmean_anom_h = [2pi*j/num_mean_anom for i in 0:(num_ecc-1), j in 0:(num_mean_anom-1) ];"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we can put these into GPUArrays and perform the calculation on the GPU using Julia's dot-syntax for broadcasting the function to each element of the arrays."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "RvPlanetParamT = Float32\necc_d = CuArray{RvPlanetParamT}(ecc_h)\nmean_anom_d = CuArray{RvPlanetParamT}(mean_anom_h)\necc_anom_d = calc_ecc_anom.(mean_anom_d,ecc_d)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can compare the time to compute the eccentric anomaly at each point in our grid using the CPU and GPU."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime ecc_anom_h = calc_ecc_anom.(mean_anom_h,ecc_h) samples=5\nCuArrays.@time CuArrays.@sync( ecc_anom_d = calc_ecc_anom.(mean_anom_d,ecc_d) )\nCuArrays.@time CuArrays.@sync( ecc_anom_d = calc_ecc_anom.(mean_anom_d,ecc_d) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the GPU was likely slower than the CPU.  We can further improve the performance by having the GPU write directly to a pre-allocated array, using `.=`.  While this will give us a speed boost, the calculation is performed entirely on the GPU and that comes with some limitations.  In particular, the default `sin` and `sqrt` functions have error handling built in that isn't compatible with the GPU.  The [CUDAnative.jl](https://github.com/JuliaGPU/CUDAnative.jl) package provides access to numerous CUDA-specific function that we can use to work around this problem.  We'll simply  replace calls to `sin(x)` with calls to `CUDAnative.sin(x)` (and similar for `sqrt`).  That's been implemented that in a similar function calc_ecc_anom_gpu, so you can see how to use this technique for your project."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ecc_anom_d = cuzeros(RvPlanetParamT,size(mean_anom_d))\nCuArrays.@time CuArrays.@sync( ecc_anom_d .= calc_ecc_anom_gpu.(mean_anom_d,ecc_d) )\nCuArrays.@time CuArrays.@sync( ecc_anom_d .= calc_ecc_anom_gpu.(mean_anom_d,ecc_d) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fun, we can plot our results for the eccentric anomaly as a function of mean anomaly and eccentricity."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ecc_anom_d .= calc_ecc_anom_gpu.(mean_anom_d,ecc_d)\nGPUArrays.synchronize(ecc_anom_d)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#using Plots   # In case you didn't load Plots earlier\n#pyplot()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(Array(mean_anom_d[1,:]), Array(ecc_d[:,1]), ecc_anom_d, st = :contourf)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimizing launch overhead in GPU calculations: Kernel fusion\nOf course, we usually want to perform more complicated calculations than just solving one equation.  For example, we might want to solve Kepler's equation so that we can compute the radial velocity of a star under a given model and compare that to observations.  Think back to the time required to launch a GPU kernel compared to the time to perform it's work.  If we launch lots of small kernels and/or constantly move data back and forth between the GPU and CPU memory systems, we won't be able to realize the computational benefits of a GPU.  Therefore, we'll want to \"fuse\" multiple operations into one kernel and keep most results on the GPU and only move back the final results we need. The following example demonstrates this programming pattern.\n\nWhile I put the code for solving Kepler's equation in a separate file, now I'd like you to pay closer attention to syntax of writing functions for a GPU, so I've left the code for computing the radial velocity due to a planet on a Keplerian orbit in the notebook."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"   calc_rv_one_planet(P,K,h,k,M0, time) \nCalculate radial velocity of star due to planet following Keplerian orbit at specified time.\nInputs:\n- Period\n- RV amplitude\n- h = e sin(omega)\n- k = e cos(omega)\n- mean anomaly at time=0\n- time\nThis differs from usual expressions so as to be differntiable, even at zero eccentricity.  \nBased on \"An analytical solution for Kepler's problem\"\nPál, András, Monthly Notices of the Royal Astronomical Society, 396, 3, 1737-1742.  2009MNRAS.396.1737P\n\"\"\"\nfunction calc_rv_one_planet(P::T1,K::T1,h::T1,k::T1,M0::T1, time::T2)  where {T1<:Number, T2<:Number}\n  ecc = sqrt(h*h+k*k)\n  w = atan(k,h)\n  n = T1(2pi)/P\n  M = time*n-M0\n  E = calc_ecc_anom(M,ecc)\n  lambda = w+M\n  lpesE = lambda+ecc*sin(E)  \n  c = cos(lpesE)\n  s = sin(lpesE)\n  @assert(zero(ecc)<=ecc<one(ecc))\n  j = sqrt((one(T1)-ecc)*(one(T1)+ecc))\n  p, q = (ecc == zero(T1)) ? (zero(T1), zero(T1)) : (ecc*sin(E), ecc*cos(E))\n  a = K/(n/sqrt((one(T1)-ecc)*(one(T1)+ecc)))\n  zdot = a*n/(one(T1)-q)*( cos(lambda+p)-k*q/(one(T1)+j) )\nend\n\nfunction calc_rv_one_planet_gpu(P::T1,K::T1,h::T1,k::T1,M0::T1, time::T2) where {T1<:Float32, T2<:Number}\n  ecc = CUDAnative.sqrt(h*h+k*k)\n  n = T1(2pi)/P\n  M = mod(time*n-M0,T2(2pi))  # Allow for modulus operation to be in Float64 if times are\n  M = T1(M)                   # Once in small range convert to Float32 for speed\n  E = calc_ecc_anom_gpu(M,ecc)\n  w = CUDAnative.atan2(k,h)\n  lambda = w+M   \n  lpesE = lambda+ecc*CUDAnative.sin(E)\n  c = CUDAnative.cos(lpesE)\n  s = CUDAnative.sin(lpesE)\n  @assert(zero(ecc)<=ecc<one(ecc))\n  j = CUDAnative.sqrt((one(T2)-ecc)*(one(T2)+ecc))\n  p, q = (ecc == zero(T2)) ? (zero(T2), zero(T2)) : (ecc*CUDAnative.sin(E), ecc*CUDAnative.cos(E))\n  a = K*P/T1(2pi)*CUDAnative.sqrt((one(T2)-ecc)*(one(T2)+ecc))\n  zdot = a*n/(one(T2)-q)*( CUDAnative.cos(lambda+p)-k*q/(one(T2)+j) )\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, notice that each function takes a list of scalars.  We'll use Julia's dot broadcasting to operate on arrays.  Previously, we discussed the danger of having a function take a long list of arguments.  Consider the functions above a low-level interface that a GPU programmer would write and test once, but users wouldn't use.  Then we can reduce the risk of problems and make the code more convenient by defining a composite structure to contain the planet's RV parameters, along with helper functions that unpack the composite type and call the low-level function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct RvPlanetParam{T<:Number}\n   P::T\n   K::T\n   h::T\n   k::T\n   M0::T\nend\n\nfunction calc_rv_one_planet(theta::RvPlanetParam{T1}, time::T2) where {T1<:Number,T2<:Number}\n  P,K,h,k,M0 = theta.P, theta.K, theta.h, theta.k, theta.M0\n  calc_rv_one_planet(P,K,h,k,M0, time)\nend\n\nfunction calc_rv_one_planet_gpu(theta::RvPlanetParam{T1}, time::T2) where {T1<:Number,T2<:Number}\n  P,K,h,k,M0 = theta.P, theta.K, theta.h, theta.k, theta.M0\n  calc_rv_one_planet_gpu(P,K,h,k,M0, time)\nend\n\nfunction calc_rv_one_planet_gpu_fast(theta::RvPlanetParam{T1}, time::T2) where {T1<:Number,T2<:Number}\n  P,K,h,k,M0 = theta.P, theta.K, theta.h, theta.k, theta.M0\n  calc_rv_one_planet_gpu_fast(P,K,h,k,M0, time)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we could create a nice interface for setting and accessing the data in `RvPlanetParms`.  At the same time, this pattern allows both the CPU and the GPU to store and access to the data in a very efficient way.  In particular, Julia's dot broadcasting allows us to pass separate arrays for each parameter.  For example,"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "RvPlanetParam.([4.0,5.0],[10.0,9.0],[0.05,0.05],[-0.1,0.1],[0.1*pi,0.2*pi])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a set of simulated observations times over ~8 months.  This time, instead of generating the data on the CPU and moving it to the GPU, we'll generate the random values on the GPU (using the `CuArrays` interface to the [CURAND](https://docs.nvidia.com/cuda/curand/index.html) library from the CUDA toolkit), perform arithmetic on the GPU, and then sort the resulting times, all leaving the data on the GPU."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import CuArrays: CURAND.curand, CURAND.curandn"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "num_obs = 200\nmin_obs_time = 0.0f0\nmax_obs_time = 240.0f0\ntimes_d = min_obs_time .+ (max_obs_time-min_obs_time) .* curand(Float64,num_obs) \nsort!(times_d)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And definte the true parameters for a simulated data set."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "P_true = 4.0\nK_true = 10.0\nh_true = 0.05\nk_true = 0.10\nM0_true = pi/4\nsigma_obs = 1.0f0\nparam_h = [RvPlanetParam{RvPlanetParamT}(P_true,K_true,h_true,k_true,M0_true)]\nparam_d = cu(param_h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute the radial velocities under the true model, either on the GPU."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CUDAnative"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rvs_true_d = cuzeros(Float32,num_obs)\nrvs_true_fast_d = cuzeros(Float32,num_obs)\n#@code_warntype  calc_rv_one_planet_gpu.(param_d,times_d)\nrvs_true_d = calc_rv_one_planet_gpu.(param_d,times_d)\n#rvs_true_fast_d = calc_rv_one_planet_gpu_fast.(param_d,times_d)\nrvs_obs_d = rvs_true_d .+ sigma_obs .* curandn(RvPlanetParamT,num_obs)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we passed an array of RvPlanetParam, so we're using good programming practices, and our helper function allows Julia to convert that into call the low-level function for the GPU.  We can bring the results back to the CPU using `Array`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rvs_obs_h = Array(rvs_obs_d)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often, we need to explore a large parameter space to search for a good model.  For sake of an example, we'll use a simple Monte Carlo search.  Of course, it is usually wise to use a more efficient algorithm."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "num_param = 10000\nPlist_d = P_true .* ( 1.0 .+ 0.01 .* (curand(RvPlanetParamT,num_param).-0.5) )\nKlist_d = K_true .* ( 1.0 .+ 0.1  .* (curand(RvPlanetParamT,num_param).-0.5) )\nhlist_d = h_true .+ 0.1 .* (curand(RvPlanetParamT,num_param).-0.5)\nklist_d = k_true .+ 0.1 .* (curand(RvPlanetParamT,num_param).-0.5)\nM0list_d = M0_true .+ 2pi .* 0.1 .* (curand(RvPlanetParamT,num_param).-0.5)\nCuArrays.@time CuArrays.@sync ( params_d = RvPlanetParam{RvPlanetParamT}.(Plist_d, Klist_d, hlist_d, klist_d, M0list_d); )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll want to evaluate the Planet's velocity at each time and for each set of model parameters.  We can acheive that using Julia's [`repeat`](https://docs.julialang.org/en/v1/base/arrays/#Base.repeat) and [`reshape`](https://docs.julialang.org/en/v1/base/arrays/#Base.reshape) functions to minimize unnecessary copying of the data, just as we would do the CPU.  It may be useful to look at the size and/or contents of the new CuArrays being being created, to help understand the upcoming function call.  \n\n(For some reason the Weave script gave an error when I left out the semi-colons.  If you remove the semi-colons, then I'd suggest restoring the semi-colons after each of the next two cells before submitting this notebook as a batch job.)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CuArrays.@time expanded_times_d = reshape(repeat(times_d,num_param),num_obs,num_param);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CuArrays.@time expanded_params_d = reshape(params_d,1,num_param);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "How much GPU memory was required for each of the above operations.  How does that compare to creating new arrays of `num_obs`x`num_param`?\n    \nINSERT RESPONSE\n\nNow, let's compute the radial velocity at each time for each set of parameters.  We'll time the results and compare to the time on the CPU.  Remember, that the first call to a function with a given set of parameter types results in an expensive compile operation on either the CPU or GPU.  So we'll run each twice."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rvs_d = cuzeros(RvPlanetParamT,num_param,num_obs)\nCuArrays.@time CuArrays.@sync( rvs_d .=  calc_rv_one_planet_gpu.(reshape(params_d,num_param,1), reshape(repeat(times_d,num_param),num_obs,num_param)' ) )\nCuArrays.@time CuArrays.@sync( rvs_d .=  calc_rv_one_planet_gpu.(reshape(params_d,num_param,1), reshape(repeat(times_d,num_param),num_obs,num_param)' ) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "params_h = Array(params_d)\ntimes_h = Array(times_d)\n@time rvs_h = calc_rv_one_planet.(reshape(params_h,num_param,1), reshape(repeat(times_h,num_param),num_obs,num_param)' )\n@time rvs_h = calc_rv_one_planet.(reshape(params_h,num_param,1), reshape(repeat(times_h,num_param),num_obs,num_param)' )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the GPU and CPU performance compare for this calculation?\n    \nINSERT RESPONSE\n\nHow large are the differences between the calculations on the CPU and GPU?  \nOnce you've run this notebook on a dedicated CyberLAMP GPU node, come back and expand on your comparison, noting any difference between the interactive and CyberLAMP nodes.\n\nINSERT RESPONSES"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "maximum(abs.(Array(rvs_d).-rvs_h))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimizing communications overhead in GPU calculations: Reductions\nSo far we've compute the velocity at each time.  Often we'd be most interested in the quality of the fit summing the differences between the model and data at each time.  We can do that, keeping all the data on the GPU using `sum`.  (GPUArrays also provided `prod` for prodcut reductions and a more generic `reduce`.)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "chisq_d = vec(sum(((rvs_d.-rvs_obs_d')./sigma_obs).^2,dims=2) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "By performing the reduction on the GPU, we get faster calculation of $\\chi^2$, but more importantly, we reduce the amount of data that would need to be sent from the GPU to the CPU's memory system."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CuArrays.@time CuArrays.@sync( chi_sq_d = sum(((rvs_d.-rvs_obs_d')./sigma_obs).^2,dims=2) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CuArrays.@time CuArrays.@sync( rvs_h = Array(rvs_d) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CuArrays.@time CuArrays.@sync( chi_sq_h = Array(chi_sq_d) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "How much longer does it take to send the array of all velocities back to the host, as compared to sending back only the $chi^2$ values?\n\nINSERT RESPONSE\n\nIn some cases, we may not even need to send all of the results back to the CPU.  For example, maybe we just want to find the best fitting model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "CuArrays.@time (chisq_good, idx_good) = findmin(chisq_d)\nCuArrays.@time good_param = params_d[idx_good]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the time required to send back just the parameters from the best-fit model (so far) compare to the time required to send back all of the values of $\\chi^2$? \n\nJust for fun, we can compare those values to the true parameters that we've kept on the host."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "println(\"True param: \",param_h[1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "println(\"Differences: \", map(fn->getfield(good_param,fn)-getfield(param_h[1],fn), fieldnames(typeof(good_param)) ) )\nprintln(\"ChiSq: \", chisq_good)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And graphically compare the predictions of the true model and the one we found, first in time and then in orbital phase."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter(times_h,calc_rv_one_planet.(param_h,times_h), markersize=2, xlabel=\"Time (d)\", ylabel=\"RV (m/s)\", label=\"True\")\nscatter!(times_h,calc_rv_one_planet.([good_param],times_h), label=\"Model\", markersize=2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter(2pi/param_h[1].P.*mod.(times_h.-param_h[1].M0,param_h[1].P),calc_rv_one_planet.(param_h,times_h), markersize=2, xlabel=\"Phase\", ylabel=\"RV (m/s)\", label=\"True\")\nscatter!(2pi/param_h[1].P.*mod.(times_h.-param_h[1].M0,param_h[1].P),calc_rv_one_planet.([good_param],times_h), label=\"Model\", markersize=2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implications for your Class Project\nNow that you've seen a tour of how to perform basic GPU computing, do you think that GPU computing would be likely to provide a significant performance benefit for the calculations in your class project?  Why or why not?\n        \nINSERT RESPONSE"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.0.2"
    },
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
