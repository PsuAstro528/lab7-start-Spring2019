{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astro 528, Lab 7, Exercise 1\n",
    "## GPU Computing I:  Getting Started & Linear Algebra\n",
    "\n",
    "In this lab exercise, we'll verify that we have access to a GPU, learn a little about its properties and benchmark some simple operations on a GPU to see how performance compares.  While most laptops have a GPU, most will either note be setup for general purpose computing or will be so low power that the benchmarking results won't be very informative.  Therefore, students are advised to run the exercises in this lab on ICS-ACI rather than their own system (unless they're confident they have setup their own system for GPU computing properly).  \n",
    "\n",
    "The ICS-ACI Jupyter notebook server and interactive desktop provided by the ICS-ACI portal are now using interactive notes that include a GPU, and students should be able to run all the code there.  However, the GPUs on the interactive nodes are relatively modest GPUs.  Therefore, after you've stepped through the notebook, you'll want to submit a PBS job, so you can run the calculations on one of the CyberLAMP GPU nodes.  For that step, you'll use the [command line interface](https://ics.psu.edu/computing-services/ics-aci-user-guide/#05-00-basics-aci-resources) to submit the PBS jobs, following a similar syntax as the [lab's 6 README](https://github.com/PsuAstro528/lab6-start/blob/master/README.md).  \n",
    "\n",
    "## Setting up Julia to use the GPU\n",
    "First, we need to make sure that your computer can find the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit).  I've installed version 9.1.0 in `/gpfs/group/ebf11/default/astro528/cuda` and all students in the class should have read access to it.  In order for other packages to know where to find it, we'll set the environment variable `CUDAHOME`.  You could do this in a dotfile (e.g., adding \n",
    "```bash \n",
    "export CUDAHOME=/gpfs/group/ebf11/default/astro528/cuda\n",
    "```\n",
    "to your `~/.bashrc` *before* you start the Jupyter notebook server or interactive desktop file.  If that worked, then the next cell should return that path.  If it didn't work, then the `else` part of the next cell should set it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CUDAHOME environment variable has not been set.  Setting it now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/gpfs/group/ebf11/default/astro528/cuda\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if haskey(ENV,\"CUDAHOME\")\n",
    "    println(\"The CUDAHOME environment variable has been set to: \",ENV[\"CUDAHOME\"])\n",
    "else\n",
    "    println(\"The CUDAHOME environment variable has not been set.  Setting it now...\")\n",
    "    ENV[\"CUDAHOME\"] = \"/gpfs/group/ebf11/default/astro528/cuda\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we've told Julia were to find the CUDA toolkit, install the pacakages we'll be using, activate the current project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/storage/home/l/len56/lab7-lenun/Project.toml\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then install the necessary packages.  This instantiate should only need to be run once, assuming everything worked.  If you get errors, then you'll likely need to make sure CUDAHOME is set correctly and then use `Pkg.build` to rebuild those libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/work/julia_depot/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "Pkg.instantiate()  # Only need to run once for the whole lab (assuming it works the first time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if you get a warning about not finding the cudnn library.\n",
    "\n",
    "## CUDAdrv:  Julia interface to the CUDA driver\n",
    "\n",
    "For this first exercise, we'll install several packages individually to see what they do.  The [CUDAdrv.jl](https://juliagpu.gitlab.io/CUDAdrv.jl/) package is a Julia wrapper for the CUDA driver that allows programs to access an NVIDIA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using CUDAdrv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides very basic functionality, like querying what version of the CUDA Toolkit is being used,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v\"9.1.0\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDAdrv.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the GPU, querying the name of the GPU being used,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quadro K4000\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_dev = CuDevice(0)\n",
    "name(gpu_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "querying how much RAM is avaliable on the GPU,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU has 2.94744873046875GB of RAM.\n"
     ]
    }
   ],
   "source": [
    "gpu_ram = Int64(totalmem(gpu_dev)) \n",
    "println(\"GPU has \",gpu_ram/1024^3, \"GB of RAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the version indicating the current GPU's [\"compute capability\"](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v\"3.0.0\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capability(gpu_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the compute capability tells you the technical sepcifications of your GPU that would be needed to do fairly low-level programming.  Rather than [looking up the specs of your graphics card online](https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units), you can query [a large list of GPU attributes](https://github.com/JuliaGPU/CUDAdrv.jl/blob/master/src/devices.jl#L60).  For example, how many multi-processors does the current GPU have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute(gpu_dev,CUDAdrv.MULTIPROCESSOR_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs execute many calculations at once, typically thousands or even hundreds of thousands threads are run in a single call to the GPU kernel.  The threads are grouped into \"blocks\" and the GPU can distribute different blocks to different multiprocessors as it sees fit.  Each blocks may be broken down into one or multiple \"warps\" which are run on the same multiprocessor, but not necessarily at the same time.  All computations in one warp are executed in parallel on a single multiprocessor.  Let's check the maximum number of threads per block and how many threads are in one warp on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute(gpu_dev,CUDAdrv.MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warpsize(gpu_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have fewer threads in a block, than the warp size, then then some of the arithmetic units in the multiprocessor won't be used effectively.  \n",
    "Nevertheless, sometimes this is necessary, because there's a fixed number of registers per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute(gpu_dev,CUDAdrv.MAX_REGISTERS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wants to get the full power of GPUs, then one is likely to do at least some low-level GPU programming in which one would need to make use of this information to choose how to divide up the work among threads and blocks efficiently.  \n",
    "\n",
    "### GPUArrays:  A High-level Julia interface to GPU programming\n",
    "\n",
    "For this exercise, we'll use the [GPUArrays.jl](https://github.com/JuliaGPU/GPUArrays.jl) package that provides a high-level interface that hides the above details from the programmer.  Different GPU manufacturers provide different functionality and libraries.  The GPUs at ICS-ACI are NVIDIA GPUs, and these are most efficiently programmed using [CUDA](https://en.wikipedia.org/wiki/CUDA).  On the other hand, AMD GPUs are most efficiently programmed using [OpenCL](https://en.wikipedia.org/wiki/OpenCL).  Some programmers perfer to use OpenCL, since CUDA is a proprietary language.  Julia has mid/high-level libraries for array operations for using either CUDA (i.e., [CuArrays.jl](https://github.com/JuliaGPU/CuArrays.jl)) or OpenCL (i.e., [CLArrays.jl](https://github.com/JuliaGPU/CLArrays.jl)).  It would be nice to be able to write code that can run with either type of GPU.  Indeed, the [GPUArrays.jl](https://github.com/JuliaGPU/GPUArrays.jl) package provides just that.  For many applications, [GPUArrays.jl](https://github.com/JuliaGPU/GPUArrays.jl) provides all you need to get orders of magnitude speed-up relative to a CPU.  \n",
    "\n",
    "Since this is our first time using these packages, let's load them one at a time, just in case there are any error messages.  Since CuArrays is the lower-level package, let's first make sure that load successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that CuArrays loaded successfuly, proceed to load the GPUArrays package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using GPUArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll benchmark some linear algebra.  Because linear algebra is so common, there are efficient [BLAS libraries](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) avaliable for both CPUs and GPU.\n",
    "Create some matrices and arrays for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1024\n",
    "A_h = randn(N,N)\n",
    "x_h = randn(N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_h = A_h*x_h;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform calculations on a GPU, we'll need to send the data from the CPU to the GPU.  The CuArrays pacakge provides a datatype `CuDeviceArray` for arrays that live on the device.  Manually specifying when to send data back and forth can be good for efficiency, but does require more care.  So instead, we use `CuArray`'s that take care of moving the data between the CPU (or \"host\") memory system and the GPU (or \"device\") memory system for us.  \n",
    "\n",
    "We can create a 'CuArray' from an existing Array simply with the `cu(.)` function.  Then we can proceed to do arithmetic on them with the same syntax as when using standard Arrays.  (Generic programming is amazing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " -55.895813 \n",
       "  35.94191  \n",
       " -10.782955 \n",
       "  30.327911 \n",
       "  14.379402 \n",
       "  -1.7469091\n",
       "  -2.6580715\n",
       " -19.46954  \n",
       "  -6.7306848\n",
       "  36.89754  \n",
       "  -8.1399355\n",
       "  54.714214 \n",
       " -28.912876 \n",
       "   ⋮        \n",
       " -49.51562  \n",
       "  45.390663 \n",
       " -36.605312 \n",
       "   8.640903 \n",
       "  -2.531618 \n",
       "  18.733774 \n",
       "  22.146948 \n",
       "  -8.0828   \n",
       "  19.638004 \n",
       "  33.10667  \n",
       " -25.65487  \n",
       " -14.046345 "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_d = cu(A_h)\n",
    "x_d = cu(x_h)\n",
    "b_d = A_d*x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note the type of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CuArray{Float32,2}, CuArray{Float32,1}, CuArray{Float32,1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(A_d), typeof(x_d), typeof(b_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a CuArray, the result calculation is also stored as a CuArray and left on the GPU.  While CuArrays let us access individual elements, if we want to look at the whole array, then it would be faster to copy all the data at once to CPU.  We can bring the full result back to the host, using `Array(.)` or `collect(.)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Array{Float32,1}:\n",
       " -55.895813 \n",
       "  35.94191  \n",
       " -10.782955 \n",
       "  30.327911 \n",
       "  14.379402 \n",
       "  -1.7469091\n",
       "  -2.6580715\n",
       " -19.46954  \n",
       "  -6.7306848\n",
       "  36.89754  \n",
       "  -8.1399355\n",
       "  54.714214 \n",
       " -28.912876 \n",
       "   ⋮        \n",
       " -49.51562  \n",
       "  45.390663 \n",
       " -36.605312 \n",
       "   8.640903 \n",
       "  -2.531618 \n",
       "  18.733774 \n",
       "  22.146948 \n",
       "  -8.0828   \n",
       "  19.638004 \n",
       "  33.10667  \n",
       " -25.65487  \n",
       " -14.046345 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_comp_h = Array(b_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compare the results.  What do you expect for the maximum difference in any element in `b`?\n",
    "        \n",
    "INSERT RESPONSE **The difference is probably on the order of ten.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7822767634688716e-5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum(b_comp_h .- b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the results compare to your expectations?  \n",
    "\n",
    "INSERT RESPONSE **The difference was really small, 3e-5.**\n",
    "\n",
    "One thing to keep in mind is that most \"consumer grade\" GPUs are designed to only perform single-precision arithmetic.  Even GPUs that do support double precission are often significantly slower at double precission arithmetic than single precission.  The difference is particularly noticable for \"consumer grade\" GPUs.  When we need double precission, we can specify that explicitly.  Below we will use double precision on the GPU and test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5631940186722204e-13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_d64 = CuArray{Float64}(A_h)\n",
    "x_d64 = CuArray{Float64}(x_h)\n",
    "b_d64 = A_d64*x_d64\n",
    "maximum(Array(b_d64) .- b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do the results compare to your expectations?  \n",
    "\n",
    "RESPONSE: **Given the previous small value, I expected this one to be even smaller! Which it was: 1.6e-13.**\n",
    "\n",
    "\n",
    "## Benchmarking GPU for Linear Algebra\n",
    "Now, we'll do some benchmarking of the CPU vs GPU, so let's load the usual BenchmarkTools package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools, Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we'll be able to use the exact same macros to benchmark code running on either the CPU or the GPU.  To keep things reasonably fast, we'll specify that we only want Julia to benchmark each calculation a few times.  \n",
    "Calls to the GPU run asynchronously.  So if we want to benchmark how long is required to the calculation to complete, we need to tell Julia to wait until the calculation has been synchronized.  We can do that either with the `@sync` macro in CuArrays or with the `synchronize(.)` function in GPUArrays to make our code more general.  `@sync` makes everyone wait until everything is complete.  `synchronize(x)` only waits until the array `x` is synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  592 bytes\n",
       "  allocs estimate:  22\n",
       "  --------------\n",
       "  minimum time:     9.210 μs (0.00% GC)\n",
       "  median time:      10.666 μs (0.00% GC)\n",
       "  mean time:        21.918 μs (0.00% GC)\n",
       "  maximum time:     66.867 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark b_d = $A_d*$x_d samples=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to launch the GPU kernel and exit?\n",
    "\n",
    "INSERT RESPONSE **It took about 10 μs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  704 bytes\n",
       "  allocs estimate:  26\n",
       "  --------------\n",
       "  minimum time:     123.390 μs (0.00% GC)\n",
       "  median time:      127.731 μs (0.00% GC)\n",
       "  mean time:        267.037 μs (0.00% GC)\n",
       "  maximum time:     582.668 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark CuArrays.@sync( b_d = $A_d*$x_d) samples=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long did it take to complete the calculation and store to to an array on the GPU?  How does this compare to the cost of launching the kernel?  What are the implications for the ammount of work you'd want per GPU call in order to make efficient use of the GPU?\n",
    "\n",
    "INSERT RESPONSE **It took about 130 μs, which is about 10% of the launch time. If you are going to use GPU, then the amount of work should be large given the launch time cost. Otherwise, it's better to use CPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.16 KiB\n",
       "  allocs estimate:  62\n",
       "  --------------\n",
       "  minimum time:     82.647 μs (0.00% GC)\n",
       "  median time:      83.255 μs (0.00% GC)\n",
       "  mean time:        94.152 μs (0.00% GC)\n",
       "  maximum time:     133.763 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark ( b_d .= $A_d*$x_d;  GPUArrays.synchronize(b_d); ) samples=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare that to double precission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.16 KiB\n",
       "  allocs estimate:  62\n",
       "  --------------\n",
       "  minimum time:     133.159 μs (0.00% GC)\n",
       "  median time:      136.351 μs (0.00% GC)\n",
       "  mean time:        147.946 μs (0.00% GC)\n",
       "  maximum time:     196.413 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark ( b_d64 .= $A_d64*$x_d64;  GPUArrays.synchronize(b_d64); ) samples=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll want to perform benchmarks for several problem sizes, I've provided a function `run_benchmarks` to make things a little easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_benchmarks"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" run_benchmarks(N; opts)\n",
    "Benchmark matrix-vector multiplication and optionally linear solve.\n",
    "Optional arguments:\n",
    "- num_samples:  How many times to perform each calculation being benchmarked\n",
    "- calc_solve: Whether to benchmark linear solve (false)\n",
    "- run_cpu:  Whether to perform benchmarks on cpu (true)\n",
    "- run_gpu:  Whether to perform benchmarks on gpu (false)\n",
    "- verbose:  Whether to print status updates.\n",
    "Output: A dictionary with results of each benchmark performed and \n",
    "        optionally the difference in final CPU and GPU calculations (if both are run).  \n",
    "\"\"\"\n",
    "function run_benchmarks(N::Integer; num_samples=3, run_cpu=true, run_gpu=false,\n",
    "                            gpu_type = Float32, mat_vec=true, mat_mat=false, compare=false, verbose=false )\n",
    "   verbose && println(\"Benchmarking with problem size = \",N)\n",
    "   A_h = randn(N,N)\n",
    "   x_h = randn(N)\n",
    "   output = Dict()\n",
    "   if run_gpu\n",
    "      A_d = CuArray{gpu_type}(A_h)\n",
    "      x_d = CuArray{gpu_type}(x_h)\n",
    "   end\n",
    "   if mat_vec\n",
    "      if run_cpu\n",
    "         verbose && println(\"Benchmarking matrix-vector multiply on CPU\")\n",
    "         # The `$` signs below specify that we want benchmark to use the local variables, rather than global variables with same name\n",
    "         output[\"cpu_mat_vec_mul\"] = @benchmark b_h = $A_h*$x_h samples=num_samples\n",
    "      end\n",
    "      if run_gpu\n",
    "         verbose && println(\"Benchmarking matrix-vector multiply on GPU\")\n",
    "         #output[\"gpu_mat_vec_mul\"] = @benchmark CuArrays.@sync( b_d = $A_d*$x_d) samples=num_samples\n",
    "         output[\"gpu_mat_vec_mul\"] = @benchmark ( b_d = $A_d*$x_d; GPUArrays.synchronize(b_d) ) samples=num_samples\n",
    "      end\n",
    "      if run_cpu && run_gpu && compare\n",
    "        b_h = A_h*x_h\n",
    "        b_d = A_d*x_d\n",
    "        output[\"diff_mat_vec_mul\"] = convert(typeof(b_h),Array(b_d)).-b_h\n",
    "      end\n",
    "   end\n",
    "   if mat_mat\n",
    "      if run_cpu\n",
    "         verbose && println(\"Benchmarking matrix-matrix multiply on CPU\")\n",
    "         # The `$` signs below specify that we want benchmark to use the local variables, rather than global variables with same name\n",
    "         output[\"cpu_mat_mat_mul\"] = @benchmark Asq_h = $A_h*$A_h' samples=num_samples\n",
    "      end\n",
    "      if run_gpu\n",
    "         verbose && println(\"Benchmarking matrix-matrix multiply on GPU\")\n",
    "         output[\"gpu_mat_mat_mul\"] = @benchmark ( Asq_d = $A_d*$A_d'; GPUArrays.synchronize(Asq_d) ) samples=num_samples\n",
    "      end\n",
    "      if run_cpu && run_gpu && compare\n",
    "        Asq_h = A_h*A_h'\n",
    "        Asq_d = A_d*A_d'\n",
    "        output[\"diff_mat_mat_mul\"] = convert(typeof(Asq_h),Array(Asq_d)).-Asq_h\n",
    "      end\n",
    "   end\n",
    "   return output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, do a quick test to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with problem size = 128\n",
      "Benchmarking matrix-vector multiply on CPU\n",
      "Benchmarking matrix-vector multiply on GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 3 entries:\n",
       "  \"diff_mat_vec_mul\" => [3.1164e-7, -3.38869e-7, 8.92729e-7, -4.53995e-7, 4.851…\n",
       "  \"gpu_mat_vec_mul\"  => Trial(26.534 μs)\n",
       "  \"cpu_mat_vec_mul\"  => Trial(3.067 μs)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "have_gpu = @isdefined GPUArrays\n",
    "benchmark_result = run_benchmarks(128,verbose=true,mat_vec=true,run_gpu=have_gpu,compare=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract the median run time for the multiplication using the CPU with expressions like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3302.125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median(benchmark_result[\"cpu_mat_vec_mul\"].times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare this to the performance using double precission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with problem size = 128\n",
      "Benchmarking matrix-vector multiply on CPU\n",
      "Benchmarking matrix-vector multiply on GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3341.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "have_gpu = @isdefined GPUArrays\n",
    "benchmark_result = run_benchmarks(128,verbose=true,gpu_type=Float64,mat_vec=true,run_gpu=have_gpu,compare=true)\n",
    "median(benchmark_result[\"cpu_mat_vec_mul\"].times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform run some benchmarks and plot the results.  After you've made your first plot, you may want to increase the range of problem sizes slightly to extend your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "#pyplot()  # in case gr() gives you trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Dict{Any,Any},1}:\n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(314.249 ns),\"gpu_mat_mat_mul\"=>Trial(28.042 μs),\"gpu_mat_vec_mul\"=>Trial(25.103 μs),\"cpu_mat_vec_mul\"=>Trial(92.156 ns)) \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(706.495 ns),\"gpu_mat_mat_mul\"=>Trial(25.633 μs),\"gpu_mat_vec_mul\"=>Trial(24.552 μs),\"cpu_mat_vec_mul\"=>Trial(93.860 ns)) \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(1.877 μs),\"gpu_mat_mat_mul\"=>Trial(26.639 μs),\"gpu_mat_vec_mul\"=>Trial(25.436 μs),\"cpu_mat_vec_mul\"=>Trial(159.969 ns))  \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(6.148 μs),\"gpu_mat_mat_mul\"=>Trial(26.746 μs),\"gpu_mat_vec_mul\"=>Trial(25.337 μs),\"cpu_mat_vec_mul\"=>Trial(294.660 ns))  \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(25.040 μs),\"gpu_mat_mat_mul\"=>Trial(34.040 μs),\"gpu_mat_vec_mul\"=>Trial(26.616 μs),\"cpu_mat_vec_mul\"=>Trial(1.169 μs))   \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(137.153 μs),\"gpu_mat_mat_mul\"=>Trial(67.901 μs),\"gpu_mat_vec_mul\"=>Trial(27.223 μs),\"cpu_mat_vec_mul\"=>Trial(3.972 μs))  \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(901.995 μs),\"gpu_mat_mat_mul\"=>Trial(103.347 μs),\"gpu_mat_vec_mul\"=>Trial(44.731 μs),\"cpu_mat_vec_mul\"=>Trial(15.827 μs))\n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(6.005 ms),\"gpu_mat_mat_mul\"=>Trial(498.860 μs),\"gpu_mat_vec_mul\"=>Trial(43.409 μs),\"cpu_mat_vec_mul\"=>Trial(66.686 μs))  \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(47.262 ms),\"gpu_mat_mat_mul\"=>Trial(3.524 ms),\"gpu_mat_vec_mul\"=>Trial(76.050 μs),\"cpu_mat_vec_mul\"=>Trial(257.461 μs))  \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(368.157 ms),\"gpu_mat_mat_mul\"=>Trial(27.215 ms),\"gpu_mat_vec_mul\"=>Trial(233.902 μs),\"cpu_mat_vec_mul\"=>Trial(3.213 ms)) \n",
       " Dict(\"cpu_mat_mat_mul\"=>Trial(2.895 s),\"gpu_mat_mat_mul\"=>Trial(214.510 ms),\"gpu_mat_vec_mul\"=>Trial(867.358 μs),\"cpu_mat_vec_mul\"=>Trial(16.200 ms))  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_sizes = [2^i for i in 2:12]\n",
    "benchmark_results32 = map( s->run_benchmarks(s,mat_mat=true,run_gpu=have_gpu,gpu_type=Float32), problem_sizes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip9000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9001\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip9001)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9002\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip9001)\" points=\"\n",
       "212.353,1440.48 2321.26,1440.48 2321.26,125.984 212.353,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9003\">\n",
       "    <rect x=\"212\" y=\"125\" width=\"2110\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  371.516,1440.48 371.516,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  868.899,1440.48 868.899,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1366.28,1440.48 1366.28,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1863.67,1440.48 1863.67,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  212.353,1235.13 2321.26,1235.13 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  212.353,903.378 2321.26,903.378 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  212.353,571.627 2321.26,571.627 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  212.353,239.875 2321.26,239.875 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.353,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.353,1440.48 212.353,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  371.516,1440.48 371.516,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  868.899,1440.48 868.899,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1366.28,1440.48 1366.28,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1863.67,1440.48 1863.67,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.353,1235.13 243.986,1235.13 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.353,903.378 243.986,903.378 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.353,571.627 243.986,571.627 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.353,239.875 243.986,239.875 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 371.516, 1494.48)\" x=\"371.516\" y=\"1494.48\">2.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 868.899, 1494.48)\" x=\"868.899\" y=\"1494.48\">5.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1366.28, 1494.48)\" x=\"1366.28\" y=\"1494.48\">7.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1863.67, 1494.48)\" x=\"1863.67\" y=\"1494.48\">10.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 188.353, 1252.63)\" x=\"188.353\" y=\"1252.63\">-6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 188.353, 920.878)\" x=\"188.353\" y=\"920.878\">-4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 188.353, 589.127)\" x=\"188.353\" y=\"589.127\">-2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 188.353, 257.375)\" x=\"188.353\" y=\"257.375\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1266.81, 73.2)\" x=\"1266.81\" y=\"73.2\">Float32</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1266.81, 1590.4)\" x=\"1266.81\" y=\"1590.4\">log_2 N</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">log_10 (Time/s)</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.039,1385.87 470.992,1403.28 669.946,1345.04 868.899,1321.38 1067.85,1222.37 1266.81,1127.75 1465.76,1029.74 1664.71,865.15 1863.67,817.708 2062.62,648.443 \n",
       "  2261.57,536.606 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.039,995.938 470.992,995.551 669.946,996.884 868.899,995.988 1067.85,992.402 1266.81,991.069 1465.76,958.277 1664.71,961.945 1863.67,921.14 2062.62,840.281 \n",
       "  2261.57,745.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.039,1313.02 470.992,1257.96 669.946,1175.34 868.899,1104.22 1067.85,999.211 1266.81,879.236 1465.76,744.047 1664.71,607.577 1863.67,459.293 2062.62,311.201 \n",
       "  2261.57,163.187 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9003)\" style=\"stroke:#c271d2; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.039,993.132 470.992,995.562 669.946,993.248 868.899,995.071 1067.85,975.905 1266.81,926.825 1465.76,899.253 1664.71,787.17 1863.67,646.627 2062.62,499.502 \n",
       "  2261.57,350.765 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip9001)\" points=\"\n",
       "284.353,511.904 933.585,511.904 933.585,209.504 284.353,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  284.353,511.904 933.585,511.904 933.585,209.504 284.353,209.504 284.353,511.904 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  308.353,269.984 452.353,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 476.353, 287.484)\" x=\"476.353\" y=\"287.484\">CPU mat_vec_mul</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  308.353,330.464 452.353,330.464 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 476.353, 347.964)\" x=\"476.353\" y=\"347.964\">GPU mat_vec_mul</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  308.353,390.944 452.353,390.944 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 476.353, 408.444)\" x=\"476.353\" y=\"408.444\">CPU mat_mat_mul</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9001)\" style=\"stroke:#c271d2; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  308.353,451.424 452.353,451.424 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 476.353, 468.924)\" x=\"476.353\" y=\"468.924\">GPU mat_mat_mul</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpu_times = map(i->median(benchmark_results32[i][\"cpu_mat_vec_mul\"].times),1:length(problem_sizes))\n",
    "plt = plot(log2.(problem_sizes),log10.(cpu_times*1e-9), label=\"CPU mat_vec_mul\",xaxis=\"log_2 N\", yaxis=\"log_10 (Time/s)\", title=\"Float32\", legend=:topleft)\n",
    "if haskey(benchmark_results32[1],\"gpu_mat_vec_mul\")\n",
    "    gpu_times = map(i->median(benchmark_results32[i][\"gpu_mat_vec_mul\"].times),1:length(problem_sizes))\n",
    "    plt = plot!(log2.(problem_sizes),log10.(gpu_times*1e-9), label=\"GPU mat_vec_mul\")\n",
    "end\n",
    "if haskey(benchmark_results32[1],\"cpu_mat_mat_mul\")\n",
    "    cpu_times = map(i->median(benchmark_results32[i][\"cpu_mat_mat_mul\"].times),1:length(problem_sizes))\n",
    "    plt = plot!(log2.(problem_sizes),log10.(cpu_times*1e-9), label=\"CPU mat_mat_mul\")\n",
    "end\n",
    "if haskey(benchmark_results32[1],\"gpu_mat_mat_mul\")\n",
    "    gpu_times = map(i->median(benchmark_results32[i][\"gpu_mat_mat_mul\"].times),1:length(problem_sizes))\n",
    "    plt = plot!(log2.(problem_sizes),log10.(gpu_times*1e-9), label=\"GPU mat_mat_mul\")\n",
    "end\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Dict{Any,Any},1}:\n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(25.173 μs),\"cpu_mat_vec_mul\"=>Trial(92.706 ns)) \n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(26.148 μs),\"cpu_mat_vec_mul\"=>Trial(92.056 ns)) \n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(25.604 μs),\"cpu_mat_vec_mul\"=>Trial(153.999 ns))\n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(26.593 μs),\"cpu_mat_vec_mul\"=>Trial(268.581 ns))\n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(26.128 μs),\"cpu_mat_vec_mul\"=>Trial(862.420 ns))\n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(28.305 μs),\"cpu_mat_vec_mul\"=>Trial(3.139 μs))  \n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(46.295 μs),\"cpu_mat_vec_mul\"=>Trial(22.800 μs)) \n",
       " Dict(\"gpu_mat_vec_mul\"=>Trial(54.792 μs),\"cpu_mat_vec_mul\"=>Trial(58.674 μs)) "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_sizes = [2^i for i in 2:9]\n",
    "benchmark_results64 = map( s->run_benchmarks(s,mat_mat=false,run_gpu=have_gpu,gpu_type=Float64), problem_sizes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip9200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9201\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip9201)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9202\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip9201)\" points=\"\n",
       "252.496,1440.48 2321.26,1440.48 2321.26,125.984 252.496,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip9203\">\n",
       "    <rect x=\"252\" y=\"125\" width=\"2070\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  311.046,1440.48 311.046,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  868.664,1440.48 868.664,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1426.28,1440.48 1426.28,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1983.9,1440.48 1983.9,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  252.496,1389.02 2321.26,1389.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  252.496,1176.63 2321.26,1176.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  252.496,964.246 2321.26,964.246 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  252.496,751.858 2321.26,751.858 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  252.496,539.469 2321.26,539.469 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  252.496,327.081 2321.26,327.081 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,1440.48 252.496,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  311.046,1440.48 311.046,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  868.664,1440.48 868.664,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1426.28,1440.48 1426.28,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1983.9,1440.48 1983.9,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,1389.02 283.528,1389.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,1176.63 283.528,1176.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,964.246 283.528,964.246 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,751.858 283.528,751.858 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,539.469 283.528,539.469 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  252.496,327.081 283.528,327.081 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 311.046, 1494.48)\" x=\"311.046\" y=\"1494.48\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 868.664, 1494.48)\" x=\"868.664\" y=\"1494.48\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1426.28, 1494.48)\" x=\"1426.28\" y=\"1494.48\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1983.9, 1494.48)\" x=\"1983.9\" y=\"1494.48\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 228.496, 1406.52)\" x=\"228.496\" y=\"1406.52\">-7.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 228.496, 1194.13)\" x=\"228.496\" y=\"1194.13\">-6.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 228.496, 981.746)\" x=\"228.496\" y=\"981.746\">-6.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 228.496, 769.358)\" x=\"228.496\" y=\"769.358\">-5.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 228.496, 556.969)\" x=\"228.496\" y=\"556.969\">-5.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 228.496, 344.581)\" x=\"228.496\" y=\"344.581\">-4.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1286.88, 73.2)\" x=\"1286.88\" y=\"73.2\">Float64</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1286.88, 1590.4)\" x=\"1286.88\" y=\"1590.4\">log_2 N</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">log_10 (Time/s)</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  311.046,1392.35 589.855,1403.28 868.664,1307.48 1147.47,1197.79 1426.28,975.707 1705.09,729.163 1983.9,215.999 2262.71,163.187 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9203)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  311.046,352.442 589.855,350.783 868.664,343.302 1147.47,345.216 1426.28,347.665 1705.09,332.532 1983.9,246.053 2262.71,211.128 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip9201)\" points=\"\n",
       "324.496,390.944 968.386,390.944 968.386,209.504 324.496,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  324.496,390.944 968.386,390.944 968.386,209.504 324.496,209.504 324.496,390.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  348.496,269.984 492.496,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 516.496, 287.484)\" x=\"516.496\" y=\"287.484\">CPU mat_vec_mul</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip9201)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  348.496,330.464 492.496,330.464 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip9201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 516.496, 347.964)\" x=\"516.496\" y=\"347.964\">GPU mat_vec_mul</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpu_times = map(i->median(benchmark_results64[i][\"cpu_mat_vec_mul\"].times),1:length(problem_sizes))\n",
    "plt = plot(log2.(problem_sizes),log10.(cpu_times*1e-9), label=\"CPU mat_vec_mul\",xaxis=\"log_2 N\", yaxis=\"log_10 (Time/s)\", title=\"Float64\", legend=:topleft)\n",
    "if haskey(benchmark_results64[1],\"gpu_mat_vec_mul\")\n",
    "    gpu_times = map(i->median(benchmark_results64[i][\"gpu_mat_vec_mul\"].times),1:length(problem_sizes))\n",
    "    plt = plot!(log2.(problem_sizes),log10.(gpu_times*1e-9), label=\"GPU mat_vec_mul\")\n",
    "end\n",
    "if haskey(benchmark_results64[1],\"cpu_mat_mat_mul\")\n",
    "    cpu_times = map(i->median(benchmark_results64[i][\"cpu_mat_mat_mul\"].times),1:length(problem_sizes))\n",
    "    plt = plot!(log2.(problem_sizes),log10.(cpu_times*1e-9), label=\"CPU mat_mat_mul\")\n",
    "end\n",
    "if haskey(benchmark_results64[1],\"gpu_mat_mat_mul\")\n",
    "    gpu_times = map(i->median(benchmark_results64[i][\"gpu_mat_mat_mul\"].times),1:length(problem_sizes))\n",
    "    plt = plot!(log2.(problem_sizes),log10.(gpu_times*1e-9), label=\"GPU mat_mat_mul\")\n",
    "end\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications for your class project\n",
    "Does your project involve linear algebra on large matrices/vectors?  Does it use thousands of linear algebra calculations with smaller matrices/vectors?\n",
    "If yes to either, then discuss the implications of the results from this exercise for the suitability of using GPUs to accelerate the linear algebra in your class project.  What precission would you use?\n",
    "\n",
    "INSERT RESPONSE **Currently my project does not involve linear algebra. But I can image that I could incorporate ways to do thousands of calculations. I would use double precision in order to increase the accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
